---
layout: post
category: "machinelearning"
title:  "Google machine learning"
tags: [python, machine learning, google]
---

- TOC
{:toc}

最近，[google](https://developers.google.com/machine-learning/crash-course)在其开发网站上，公开了用于内部人员进行机器学习培训的材料，可以快速帮助了解机器学习及其框架TensorFlow。量子位提供了一个相关材料的连接[（别翻墙了，谷歌机器学习速成课25讲视频全集在此）](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247495096&idx=1&sn=cb25eec7088e96f416bc1df2a6c2df10&chksm=e8d05acadfa7d3dca298cef87ed9bf836a81d9501be6210cac5c9a2f6fdc1a4dc81b10348229&mpshare=1&scene=23&srcid=0304E57F5bWLPOV7AUtjemDr#rd)。近期会学习这个系列的材料，做一点后续的笔记。


## 课程概览

### 机器学习概念

####  简介

#### 框架处理


1. 介绍了基本框架（监督式机器学习）和及一些术语
2. 基本术语：
   - 标签：要预测的真实事物（y）
   - 特征：描述数据的输入变量（xi）。特征尽量是可量化的，比如鞋码、点击次数等，美观程度等不可量化，不适合作为特征。
   - 样本：数据的特定实例
   - 有标签样本：训练模型；无标签样本：对新数据做预测
3. 模型：将样本映射到预测标签
   - 回归模型：预测连续值
   - 分类模型：预测离散值

#### 深入了解机器学习

1. 介绍了线性回归，房屋面积预测销售价格的例子，引出如何评价线性回归的好坏。
2. 模型训练：通过有标签样本来学习（确定）所有权重和偏差的理想值
3. 监督式学习（经验风险最小化）：检查多个样本并尝试找出可最大限度地减少损失的模型
4. 损失：
   - L2loss（平方误差/平方损失）：预测值和标签值之差的平方
   - 均方误差 (MSE) ：每个样本的平均平方损失，平方损失之和/样本数量

#### 降低损失

1. 训练模型需要reducing loss，迭代方法很常用。
2. 迭代：根据输入和模型的随机初始值计算损失值，然后更新模型参数值，不断循环，使得损失值达到最小（试错过程）。当损失不再变化或者变化极其缓慢，可以说模型区域收敛。
3. 梯度下降：上述迭代过程存在更新模型参数的步骤，使用此方法快速寻找达到最小损失的参数（不可能把所有的参数都尝试一遍）。
4. 梯度：损失曲线在对应参数处的梯度（偏导数：有大小和方向），负梯度是对应梯度下降的。
5. 如何选取下一个点？
  - 下降梯度值 x 学习速率（步长）
6. 如果知道损失函数梯度较小，可以使用较大的学习速率，以较快的达到最小损失。
7. 优化学习速率，理解通过调节不同的速率，使得学习效率能够收敛达到最高。在降低损失的方向选取小步长（因为想要精准的达到最低损失）。小步长：梯度步长。这种优化策略：梯度下降法。
8. 权重初始化：凸形问题，可任意点开始，因为只有一个最低点；非凸形：有多个最低点，很大程度决定于起始值。
9. 批量：单次迭代中计算梯度的样本总数。一般是总的样本，但是海量数据过于庞大。
10. 随机梯度下降（SGD）：一次抽取一个样本
11. 小批量梯度下降：每批10-1000个样本，使得估算均值接近整体，且计算时间可接受

#### 使用TF的基本步骤

1. TF(TensorFlow) API(建议从高级API开始使用):
  - 面向对象的高级API：estimator
  - 库：tf.layers, tf.losses, tf.metrics
  - 可封装C++内核的指令：TensorFlow python/C++
  - 多平台：CPU/GPU/TPU
2. TF线性回归：

```python
# tf.estimator API

import tensorflow as tf

# set up a classifier
classifier = tf.estimator.LinearClassifier()

# Train the model on some example data.
# what does steps mean here?
classifier.train(input_fn=train_input_fn, steps=2000)

# Use it to predict.
predictions = classifier.predict(input_fn=predict_input_fn)
```

3. 常用参数：
 - steps：训练迭代的总次数。一步计算一批样本产生的损失，然后使用该值修改模型权重
 - batch size：单步的样本数量，如SGD批次大小为1.
4. 通过[notebook](https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=firststeps-colab&hl=zh-cn)介绍使用TF实现线性回归模型预测房屋价格

#### 泛化

1. 泛化（Generalization）：构建的模型拟合新数据的能力。希望是泛化能力强。
2. 泛化能力弱的一个例子：过拟合。比如用复杂的非线性模型区分垃圾邮件，由于过拟合，对于新的邮件容易错误的标记为正常的或者垃圾邮件。
3. 如何提高泛化能力？对数据样本进行抽样进行训练，同时用测试样本进行模型的测试（使用测试集方法，评估模型是否出色）。
4. “奥卡姆剃刀”：模型应尽可能简单
5. 这个做法基于3点：1）从分布中随机抽取独立同分布的样本；2）分布是平稳的（不会随时间变化）；3）始终从同一分布中抽取样本（包括训练集、验证集和测试集）
6. 例子：比如购物行为在节假日和夏季不同（不平稳性），不同品种的小狗一样可爱（不是相同的分布）
7. 如果某个模型不能很好地泛化到新数据，可能是过拟合了。如果已经通过测试集数据的方式进行模型训练了？还是过拟合的问题吗？

#### 训练集和测试集

1. 测试集：评估根据训练集开发的模型的数据集
2. 陷阱：损失很低（效果很好，准确率100%） =》 检测是否不小心对测试数据进行了训练。
3. 测试集条件：1）足够大（具有统计意义）；2）能代表整个数据集（特征与训练集相同）。
4. ~~任务1：当测试损失值和训练损失值基本稳定时，其大小不存在明显差异（0.003，0.03，0.06，0.042，0.003）；任务2：学习速率3 -》1，损失差值边大；~~
5. 任务1：当学习速率=3时，测试损失明显高于训练损失；任务2：如果降低学习速率，测试损失会减小，以接近于训练损失；任务3：降低训练样本比例，大幅减少训练样本个数。

#### 验证

1. 测试集的评估结果用于指导和调整模型参数（比如学习速率和特征），是否存在问题？多次重复会拟合特定的测试集，因为这里我们只对数据集进行了一次划分。
2. 引入验证集：
   - 训练集+测试集：训练集训练 -》测试集评估 -》根据测试集效果调整 -》选择在测试集上的最佳模型。
   - 训练集+验证集+测试集：训练集训练 -》验证集评估 -》根据验证集效果调整 -》选择在验证集上的最佳模型 -》使用测试集确定效果。如果测试集效果和验证集效果不相当，则说明可能对于验证集进行了过拟合。有效防止过拟合是因为暴露给测试集的信息更少。

#### 表示法

表示：Representation

1. 选择最优代表性的数据特征集合
2. 特征工程：从原始数据提取特征，占据75%的时间。
3. 原始数据-》特征向量：
   - 映射数值：实值 =》数值(直接复制)，
   - 映射分类值：字符串 =》**独热编码**（比如编码ATCG四个碱基的，其中一个为1，其他均为0）转化为特征向量。
   - 原本可以顺序数字映射，比如不同地名映射为1，2，3。。。，但是这个涉及到权重问题，所以创建二元向量如[0,1,0,0,...]。独热编码：只有一个值为1；多热编码：多个值为1。
   - 稀疏表示法：假如有100万个街道，直接创建100万个元素的二元向量？低效！可采用稀疏表示法：仅存储非零值。
   
4. 好特征：
   - 1）具有非零值且出现多次（否则应该被过滤掉）；
   - 2）具有明确清晰的定义，比如年龄不应该以秒来计算；
   - 3）不应使用“神奇”的值，比如某房屋没有出售可定义为-1；
   - 4）特征的定义不随时间变化（比如数据是从上游传来的，则可能发生变化），保证数据的平稳性；
   - 5）不包含离群值，可监测去除，也可采用分箱技术划分区间再进行独热编码；

5. 了解数据：
   - 1）可视化；
   - 2）调试：重复样本、缺失值、离群值、与信息中心一致？训练数据与验证数据相似？
   - 3）监控：特征分位数、样本数量是否随着时间而变化？
   
6. 检查数据：1）遗漏值；2）重复样本；3）不良标签；4）不良特征值。

7. 数据清理：
   - 1）缩放特征值（标准化）：将浮点特征值从自然范围（例如 100 到900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。好处：梯度下降法更快速地收敛；避免“NaN 陷阱”；3）为每个特征确定合适的权重。常规：Z-score线性缩放
   - 2）处理极端离群值：取对数；限定最大最小特征值（winsorazation）。
   - 3）分箱：比如房屋的纬度是浮点值，这个值与房屋出售价格没有线性关系。可以对纬度进行分区（分箱，用独热编码表示为多元二维向量），就可以把纬度的权重体现出来了。

#### 特征组合
11. 正则化：简单行
12. 逻辑回归
13. 分类
14. 正则化：稀疏性
15. 神经网络简介
16. 训练神经网络
17. 多类别神经网络
18. 嵌入

### 机器学习工程

1. 生产环境机器学习系统
2. 静态训练与动态训练
3. 静态推理与动态推理
4. 数据依赖关系

### 机器学习现实世界应用示例

1. 癌症预测
2. 18世纪文学
3. 现实世界应用准则

### 总结

1. 后续步骤

