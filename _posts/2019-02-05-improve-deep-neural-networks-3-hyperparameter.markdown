---
layout: post
category: "machinelearning"
title:  "【2-3】深度学习的超参数优化、batch归一化"
tags: [python, machine learning]
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### 目录

- TOC
{:toc}

---

### 调试过程

* 超参数：
	* 某些超参数比其他的更重要
	* 【1】需调试的最重要的超参数：
		* 学习速率
	* 【2】仅次于$$\alpha$$的参数：
		* Momentum参数$$\beta$$：一般使用默认值0.9
		* 隐藏单元的数目
		* mini-batch size
	* 【3】其他因素：
		* 隐藏层数目
		* 学习速率的衰减率
	* 【4】最不重要
		* Adam算法里面的参数：$$\beta_1, \beta_2, \varepsilon$$，一般使用默认值：0.9，0.999，10^-8
* 如何选择超参数呢？
	* **核心1：随机选择点，不要网格搜索**
	* 网格搜索：当参数的数量相对较少时，很实用
	* 随机选择：深度学习领域推荐使用此方法
	* 为啥随机搜索？
		* 比如只有2个参数：学习速率$$\alpha$$和Adam参数$$\varepsilon$$，显然前者比后者重要得多。
		* 当选择同等数量的点进行调试时，【1】网格搜索：实验了5个不同的$$\alpha$$，但是$$\varepsilon$$取任何值效果是一样的。【2】随机选择：可能取到了独立的25个$$\alpha$$值，就更可能发现选择哪一个更好。
	* ![](http://www.ai-start.com/dl2017/images/75bfa084ea64d99b1d01a393a7c988a6.png)
	* 实际：三个甚至更多的超参数
	* **随机取值而不是网格取值表明，你探究了更多重要的超参数的潜在值**

	* **核心2：由粗糙到精细的策略**
	* 比如在两个参数时，随机取了一些点，发现某个区域整体效果都还不错
	* 接下来可以放大这块区域，然后更密集的随机取值

---

### 为超参数选择合适的范围

* 随机取值：
	* 在超参数范围内可提升搜索效率
	* 但不是均匀取值
	* 而是选择合适的尺寸
* 有的参数可以随机（均匀）选取：
	* 在范围内可以随机均匀选取
	* 比如隐藏单元数目、层数 ![](http://www.ai-start.com/dl2017/images/3b45624120e77aea2fcd117cbfdc9bdb.png)
* 有的参数不能随机（均匀）选取：
	* 不能线性的随机取
	* 比如学习速率$$\alpha$$：假设其范围是0.0001-1之间有最优，如果画一条范围直线随机均匀选取，则90%选取的值在0.1-1之间，10%在0.0001-0.1之间，这看上去是有问题的。
	* 可用**对数尺度**去搜索：不使用线性的，依次取0.0001,0.001,0.01,0.1,1，然后在此对数数轴上均匀选取，这样资源才比较均等
	* 具体：在$$[10^a,10^b]$$之间取值，需要做的就是在[a,b]之间均匀取值（比如为r），然后取的超参数值=$$10^r$$ ![](http://www.ai-start.com/dl2017/images/a54d5ea6cd623f741f75e62195f072ca.png)
	* 在对数坐标下取值，取最小的对数就得到a的值，取最大的对数就得到b值。在a，b间随意均匀的选取r值，再设置为$$10^r$$即可。

	* 如何给Momentum的$$\beta$$取值？**指数的加权平均**
	* 假如搜索区间是0.9-0.999
	* 不能用线性轴，可探究$$1-\beta$$
	* 当$$\beta：0.9-0.999$$时，$$1-\beta：0.1-0.001$$，换为对数轴取值，在[-3,-1]之间均匀取值为r，然后超参数值$$1-\beta=10^r，\beta=1-10^r$$ ![](http://www.ai-start.com/dl2017/images/2e3b1803ab468a94a4cae13e89217704.png)

---

### 超参数调试：熊猫 vs 鱼子酱

* 深度学习应用领域广泛，但是超参数的设定会随时间变化而变化，比如添加了新的数据等。建议：至少每隔几个月，重新测试或评估模型的超参数，以确保对数值已然很满意。
* 如何搜索超参数？
	* 两种流派 ![](http://www.ai-start.com/dl2017/images/a361c621a9a0a1a99b03eef8716c5799.png)
	* **照看模式**：随机化选择参数，一次实验一个，看效果。有的时候损失随着参数的更新一直在下降，突然某个时候可能又上升了，
	* 

---

### 参考

* [第三周：超参数调试、Batch正则化和程序框架](http://www.ai-start.com/dl2017/html/lesson2-week3.html)

---




