---
layout: post
category: "machinelearning"
title:  "【2-3】深度学习的超参数优化、batch归一化"
tags: [python, machine learning]
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### 目录

- TOC
{:toc}

---

### 调试过程

* 超参数：
	* 某些超参数比其他的更重要
	* 【1】需调试的最重要的超参数：
		* 学习速率
	* 【2】仅次于$$\alpha$$的参数：
		* Momentum参数$$\beta$$：一般使用默认值0.9
		* 隐藏单元的数目
		* mini-batch size
	* 【3】其他因素：
		* 隐藏层数目
		* 学习速率的衰减率
	* 【4】最不重要
		* Adam算法里面的参数：$$\beta_1, \beta_2, \varepsilon$$，一般使用默认值：0.9，0.999，10^-8
* 如何选择超参数呢？
	* **核心1：随机选择点，不要网格搜索**
	* 网格搜索：当参数的数量相对较少时，很实用
	* 随机选择：深度学习领域推荐使用此方法
	* 为啥随机搜索？
		* 比如只有2个参数：学习速率$$\alpha$$和Adam参数$$\varepsilon$$，显然前者比后者重要得多。
		* 当选择同等数量的点进行调试时，【1】网格搜索：实验了5个不同的$$\alpha$$，但是$$\varepsilon$$取任何值效果是一样的。【2】随机选择：可能取到了独立的25个$$\alpha$$值，就更可能发现选择哪一个更好。
	* ![](http://www.ai-start.com/dl2017/images/75bfa084ea64d99b1d01a393a7c988a6.png)
	* 实际：三个甚至更多的超参数
	* **随机取值而不是网格取值表明，你探究了更多重要的超参数的潜在值**

	* **核心2：由粗糙到精细的策略**
	* 比如在两个参数时，随机取了一些点，发现某个区域整体效果都还不错
	* 接下来可以放大这块区域，然后更密集的随机取值

---

### 为超参数选择合适的范围

* 随机取值：
	* 在超参数范围内可提升搜索效率
	* 但不是均匀取值
	* 而是选择合适的尺寸
* 有的参数可以随机（均匀）选取：
	* 在范围内可以随机均匀选取
	* 比如隐藏单元数目、层数 ![](http://www.ai-start.com/dl2017/images/3b45624120e77aea2fcd117cbfdc9bdb.png)
* 有的参数不能随机（均匀）选取：
	* 不能线性的随机取
	* 比如学习速率$$\alpha$$：假设其范围是0.0001-1之间有最优，如果画一条范围直线随机均匀选取，则90%选取的值在0.1-1之间，10%在0.0001-0.1之间，这看上去是有问题的。
	* 可用**对数尺度**去搜索：不使用线性的，依次取0.0001,0.001,0.01,0.1,1，然后在此对数数轴上均匀选取，这样资源才比较均等
	* 具体：在$$[10^a,10^b]$$之间取值，需要做的就是在[a,b]之间均匀取值（比如为r），然后取的超参数值=$$10^r$$ ![](http://www.ai-start.com/dl2017/images/a54d5ea6cd623f741f75e62195f072ca.png)
	* 在对数坐标下取值，取最小的对数就得到a的值，取最大的对数就得到b值。在a，b间随意均匀的选取r值，再设置为$$10^r$$即可。

	* 如何给Momentum的$$\beta$$取值？**指数的加权平均**
	* 假如搜索区间是0.9-0.999
	* 不能用线性轴，可探究$$1-\beta$$
	* 当$$\beta：0.9-0.999$$时，$$1-\beta：0.1-0.001$$，换为对数轴取值，在[-3,-1]之间均匀取值为r，然后超参数值$$1-\beta=10^r，\beta=1-10^r$$ ![](http://www.ai-start.com/dl2017/images/2e3b1803ab468a94a4cae13e89217704.png)

---

### 超参数调试：熊猫 vs 鱼子酱

* 深度学习应用领域广泛，但是超参数的设定会随时间变化而变化，比如添加了新的数据等。建议：至少每隔几个月，重新测试或评估模型的超参数，以确保对数值已然很满意。
* 如何搜索超参数？
	* 两种流派 ![](http://www.ai-start.com/dl2017/images/a361c621a9a0a1a99b03eef8716c5799.png)
	* **照看模式**：随机化选择参数，一次实验一个，看效果。有的时候损失随着参数的更新一直在下降，突然某个时候可能又上升了。
	* 熊猫方式：孩子很少，一次通常就一个
	* **同时实验多种模型**：平行实验许多不同的模型，最后快速选择效果最好的那个。
	* 鱼子酱模式：繁殖的时候会产生很多卵

	* 选择哪种？
	* 取决于拥有的计算资源
	* 如果足够多，选择鱼子酱模式

---

### 归一化网络的激活函数

* Batch归一化
	* 深度学习一个重要的算法
	* 使得参数搜索问题变得容易
	* 神经网络对超参数的选择更稳定
	* 超参数的范围更庞大
* **特征值**的归一化
	* 特征值的归一化：减去平均值，除以方差
	* 逻辑回归或者神经网络都可以用到这个
	* 能加快学习过程
	* 把学习的轮廓从很长的东西变成很圆的 ![](http://www.ai-start.com/dl2017/images/7eed1a2ef94832c54d1765731a57b2b5.png)
* **激活值**能否归一化？
	* 对于更深的模型
	* 特征值：输入层到第一个隐藏层
	* 激活值：其他隐藏层之间的传递值，每一层的激活值a作为下一层的输入
	* 试想：特征值的归一化是可以提高训练速度的，那么如果激活值也是归一化的，是不是也能提升训练速度呢？![](http://www.ai-start.com/dl2017/images/20949eb2c30cb22ab87eef411f01375d.png)
* **z值**的归一化：
	* 我们通常选择归一化z，就是激活函数的输入值，而不是激活后的值【激活前z值的归一化】
	* 把隐藏层的z值标准化，化为平均值为0和标准单位方差的值 ![](http://www.ai-start.com/dl2017/images/e10ea98faa1ce9fe86ec8bf9f4fef71e.png)
	
	* 是否一定需要归一化为：均值为0，单位方差？
	* 不一定！如果分布是有意义的，可以选择其他的值。
	* 做法：不直接使用归一化的值，可以在归一化的值进行参数添加以控制，$$\widetilde{z}^{(i)} = \gamma z_{norm}^{(i)}+\beta$$，这里控制的两个参数是需要进行设定的，就像Momentum里面的参数一样，也会在梯度下降中进行更新
	* 可以看到，当满足：$$\gamma=sqrt(\sigma^2+\epsilon), \beta=\mu$$时，就是上面的均值为0，标准方差的归一化了 ![](http://www.ai-start.com/dl2017/images/6216dd90d3483f05f08bd8dc86fc7df6.png)
	
	* **特征输入、隐藏单元区别：一般特征输入是归一化到平均值为0方差为1，但是隐藏神经元可以不一定是这个值**，这一点就是通过引入的两个参数进行控制的
	* 比如对于sigmoid函数，不像其归一化后的值总是在0附近，因为这样是趋于线性的

---

### Batch归一化拟合到神经网络中

* Batch归一化是发生在计算`z`和`a`之间的：
	* 输入层特征值通过与第一层之间的w、b参数，得到第一层的z
	* 然后对z进行归一化，得到$$\widetilde{z}$$
	* 对$$\widetilde{z}$$用激活函数计算a
	* 以此类推 ![](http://www.ai-start.com/dl2017/images/55047d3da405778b6272e6722cd28ac6.png)
	* 注意：这里每一层都是单独归一化的，所以每一层都有自己的参数：$$\\gamma, \beta$$
* Batch归一化通常和mini-batch一起使用
	* 每一次对一个batch进行前向传播、归一化、反向传播、梯度下降等
	* 参数b是被减去的:$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$，归一化需要均值为0标准方差，再由参数$$\gamma, \beta$$进行缩放。所以$$b^{[l]}$$的值是被减去的。通过参数$$\beta^{[l]}$$进行控制，产生偏移或偏置。![](http://www.ai-start.com/dl2017/images/3711c11e52433b6417227e78b998e849.png)
* 梯度下降更新：
	* 对于batch归一化涉及的参数也要进行更新 ![](http://www.ai-start.com/dl2017/images/797be77383bc6b34ddd2ea9e49688cf6.png)

---

### Batch归一化为什么有效？

* 原因1：类比特征值归一化
	* 做特征值归一化类似的工作，只不过对象是隐藏单元值
* 原因2：权重比网络更滞后或更深层
	* 比如第10层的权重相比第1层的权重更能经受住变化（分布的变化等）
	
	* 背景是：数据分布的变化对于网络使有影响的，需要重新许梿学习算法
	* 例子：左边训练黑猫的模型，训练好了，现在应用于有色猫的识别 ![](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190821004530.png)
	* covariate shift：如果已经学习了x到y的映射，如果x的分布改变了，那么可能需要重新训练你的学习算法

	* 为什么在神经网络中这是个问题？
	* 某一层的激活值，在计算上直接与前一层的值有关，但是如果前面一些层的参数都发生了改变，那么这些隐藏单元的值也是在不断改变的，就有了上面的covariate shift问题
	* **batch归一化：减少隐藏值分布的变化**，也许具体的z的值会变化，但是至少均值和方差分别是0和1.因此可以减少值的改变的问题，使这些值更加的稳定。![](http://www.ai-start.com/dl2017/images/db19ef11d7b654cc020a5f943806ac38.png)
	* 浅层不会左右移动那么多，因为被同样的均值和方差所限制，从而使得后层的学习更加容易。

* Batch归一化还有正则化的效果：
	* 每个mini-batch上进行归一化，由均值和方差进行缩放
	* 均值和方差是有一些噪声的：因为只是某一个mini-batch所计算出来的
	* **结果：在隐藏层的激活值上增加了噪音**。标准偏差的缩放和减去均值带来的额外噪音。
	* dropout：也是增加噪音，隐藏单元已一定的概率乘以0或者1，应用较大的mini-batch可以减少正则化效果。
	* 分析：标准偏差的缩放和减去均值带来的额外噪音。使得后部分的神经元不过度依赖于任何一个隐藏单元，所有有轻微的正则化效果。效果没有dropout那么强。

---

### 

### 参考

* [第三周：超参数调试、Batch正则化和程序框架](http://www.ai-start.com/dl2017/html/lesson2-week3.html)

---




