---
layout: post
category: "machinelearning"
title:  "损失函数"
tags: [python, machine learning]
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### 目录

- TOC
{:toc}

---

## 什么是损失函数

* 估量模型的预测值f(x)与真实值Y的不一致程度
* 损失函数越小，一般就代表模型的鲁棒性越好
* 损失函数指导模型的学习

---

## 分类损失函数

---

### 0-1 loss

* 公式：$$L(y_i, f(x_i)) = \begin{cases}
0& y_i = f(x_i)\\
1& y_i \neq f(x_i)
\end{cases}$$

* 直接比较预测值和真实值是否相等
* 预测和标签相等，值为0(预测很准，loss很小)，值为1(预测很不准，loss很大)
* 缺点：无法对x进行求导，所以在深度学习的反向传播中，不能被直接使用（一般用于启发新的loss）

---

### 负对数似然损失

* 公式：$$loss(x,f(x)) = -log(f(x))$$
* 惩罚预测的概率值小的，激励预测的概率值大的
* 预测的概率值越小，对数log值的值越小（负的越多），加一个负号，就是值越大，那么此时的loss也越大
* pytorch：`torch.nn.NLLLoss`

---

### 交叉熵

* 熵：物理学上表示一个热力学系统的无序程度
* 信息熵：
	* 信息的量化度量，由香农提出
	* **对数函数**表示对不确定性的测量
	* 熵越高，表示能传输的信息越多，熵越少，表示传输的信息越少。熵 =》信息量
	* 原理：每个信息都存在冗余，冗余的大小与符号的出现概率或者不确定性有关。出现概率大，则不确定性小，可用对数函数表征。
* 为什么对数函数？
	* 不确定性必须是出现概率的单调递减函数
	* 离散的独立事件，其总的不确定性等于各自不确定性之和
	* 不确定性：$$f=log(\frac{1}{p})=-log(p)$$
* 信息熵：
	* 单个符号的不确定性的统计平均
	* 公式：$$-\sum_{i=0}^{n} p_ilog(p_i)$$
* 分类交叉熵：
	* 所有样本在每个类别的信息熵的总和
	* 公式：$$l(f, y)=-\sum_{i}^{n}\sum_{j}^{m}y_{ij}logf(x_{ij})$$
	* 参数 `n`：样本数量
	* 参数 `m`：类别数量
	* 参数 $$y_{ij}$$：第`i`个样本属于分类`j`的标签，它是0或者1
	* 参数 $$f(x_{ij})$$：样本`i`预测为`j`分类的概率
* 特点：
	* 主要用于学习数据的概率分布
	* 像MSE等是惩罚预测错误的，交叉熵对于高可信度预测错误的会有更大的惩罚
	* 负对数损失：不会根据预测的可信度进行惩罚；交叉熵：也会惩罚**预测错误且可信度很高**的，或者**预测正确且可信度很低**的
* pytorch：`torch.nn.CrossEntropyLoss`

---

### softmax loss

* 如果上面的$$f(x_{ij})$$是softmax概率的形式（指数概率），此时就是softmax with cross-entropy loss，简称softmax loss
* softmax loss是交叉熵的一个特例
* 分类分割任务
* 不平衡样本：weighted softmax loss， focal loss
* 蒸馏学习的soft softmax loss

---

### KL散度

* 估计两个分布的相似性
* 公式：$$D_{kl}(p\mid q) = \sum_{i} p_i log(\frac{p_i}{q_i})$$
* 当p和q处处相等时，上式才=0，即分布相同
* 变形：$$D_{kl}(p\mid q) = \sum_{i} (p_i logp_i - p_i logq_i) = -l(p,p) + l(p,q)$$
* `l(p,p)`: `p`的熵，当一个分布一定时，熵为常数值
* `l(p,q)`: `p`和`q`的交叉熵
* **KL散度和交叉熵相差一个常数值**，所以用哪个作为loss都是可以的，最小化交叉熵也是最小化KL散度
* 不是根据预测的可信度进行惩罚（这是交叉熵要做的），而是根据预测和真实值的差异进行惩罚
* 注意：KL的非对称性，$$D_{kl}(p\mid q) \neq D_{kl}(q\mid p)$$
* pytorch：`torch.nn.KLDivLoss`

---

### hinge loss

* 主要用于SVM，解决间距最大化问题
* 公式：$$l(f(x), y) = max(0, 1-yf(x)) = \begin{cases}
0& y_i = f(x_i)\\
1& y_i \neq f(x_i)
\end{cases}$$

---

### hinge embedding loss

* 用于衡量两个输入是否相似或者不相似
* 公式：$$l_n = \begin{cases}
x_n& y_n = 1\\
max\{0, margin - x_n\}& y_n = -1
\end{cases}$$
* margin: default = 1
* pytorch：`torch.nn.HingeEmbeddingLoss`

---

### 指数损失与逻辑损失

* 指数形式，梯度比较大，主要用于Adaboost集成学习中
* 公式：$$l(f(x), y) = e^{-\beta y f(x)}$$
* 取对数形式：$$l(f(x), y) = \frac{1}{ln2} ln(1+e^{-yf(x)})$$，梯度相对平缓

---

### Cosine Embedding Loss

* 对于两个输入x1，x2，根据标签计算其cos相似性的loss
* 公式：$$l(f(x), y) = \begin{cases}
1-cos(x1,x2) & y = 1\\
max\{0, cos(x1,x2) - margin\}& y = -1
\end{cases}$$
* 相似性：$$similarity = cos(\theta) = \frac{A * B}{\mid A\mid \mid B\mid}$$
* 默认时marign=0
* 当y=1时，loss=1-cos(x1,x2)
* 当y=-1时，loss=max{0, cos(x1,x2)}。如果cos(x1,x2)>0，loss=cos(x1,x2)；如果cos(x1,x2)<0，loss=0.
* pytorch：`torch.nn.CosineEmbeddingLoss`

---

## 回归损失函数

---

### L1 loss | MAE

* 以绝对误差作为距离
* Mean absolute loss，MAE
* 公式：$$l(f(x), y) = \mid y - f(x) \mid$$
* 具有稀疏性，常作为正则项添加到其他loss中，可以惩罚较大的值
* 问题：梯度在零点不平滑，导致会跳过极小值
* pytorch: `torch.nn.L1Loss`

---

### L2 loss | MSE | 均方差 | 平方损失

* 欧氏距离：以误差的平方作为距离
* Mean Squared Loss/Quadratic Loss，MSE loss
* 公式：$$L2=MSE=\frac{1}{n} \sum_{1}^n(y_i - \overline{y_i})$$
* 平方使得放大大的loss，当模型范大的错误就惩罚它
* 也常常作为正则项
* 当预测值与目标值相差很大时, 梯度容易爆炸，因为梯度里包含了x−t。
* pytorch：`torch.nn.MSELoss`

---

### smooth L1 loss | Huber loss

* L1/L2 局限：
	* L1：梯度不平滑
	* L2：容易梯度爆炸
	* 新的综合两者有点的loss
* 公式：$$smooth_{L1} (x, f(x)) = \begin{cases}
0.5(x-f(x))^2& , \mid x-f(x)\mid < 1\\
\mid x-f(x)\mid -0.5& , otherwise
\end{cases}$$
* 当x-f(x)较小时，等价于L2 loss，保持平滑
* 当x-f(x)较大时，等价于L1 loss，可以限制数值的大小
* 与MSE相比，对于outliner更不敏感，当真实值和预测值差异较大值，此时类似于L1 loss，不像MSE loss的平方，所以可避免梯度爆炸
* pytorch：`torch.nn.SmoothL1Loss`

---

## GANs

---

### Margin Ranking Loss

* 对于两个输入x1，x2，以及一个标签y(取值1和-1的tensor)，评估x1和x2的排序
* 当y=1，x1的排序高于x2
* 当y=-1，x1的排序低于x2
* 公式：$$loss(x, y) = max(0, -y * (x1-x2) + margin)$$
* 如果x1、x2的排序和数据是吻合的，那么此时y * (x1-x2)是大于0的，-y * (x1-x2) + margin是小于0的，整个loss取值为0.
* 如果x1、x2的排序和数据是不吻合的，那么此时y * (x1-x2)是小于0的，-y * (x1-x2) + margin是大于0的，整个loss取值为大于0的一个值，相当于对这种错误的预测有一个惩罚.
* pytorch：`torch.nn.MarginRankingLoss`

## 参考

* [深度学习中常用的损失函数](https://zhuanlan.zhihu.com/p/60302475)
* [机器学习-损失函数](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/)
* [A Brief Overview of Loss Functions in Pytorch](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7)
* [pytorch loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)


---



