---
layout: post
category: "machinelearning"
title:  "LSTM"
tags: [python, machine learning]
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### 目录

- TOC
{:toc}

---

### RNN

* 背景：传统神经网络不具有记忆功能
	* 记忆：比如人的思考是基于记忆的知识的
* 方案：RNN
	* 将信息持久化
	* 可以解决网络的记忆功能
	* RNN可以看做是同一个网络的多个副本，每一份都将信息传递到下一个副本
	
	* RNN环式结构展开为链式：![](https://www.yunaitong.cn/media/14585445702525/14585464218639.png)
	* 链式结构展示了RNN与序列和列表的密切关系
	* 语音识别、语言建模、翻译、图片标题取的很好效果

---

### RNN不能解决长期依赖问题

* RNN核心：将以前的信息连接到当前任务中
* 缺点：
	* **不能保留更远的上下文信息**
	* **存在梯度消失和梯度爆炸的问题**。当每层的权重W小于1时，误差传到最开始的层，结果接近于0，梯度消失；当每层的权重W大于1时，误差传到最开始的层，结果变得无穷大，梯度爆炸。
* 简单例子：
	* 输入：“the clouds are in the xxx”
	* 预测：xxx
	* RNN：可以训练得很好预测出来，根据上下文，很容易
	* 特点：**当前位置与相关信息所在位置之间的距离相对较小**
* 复杂例子：
	* 输入：“I grew up in France... I speak fluent xxx”
	* 预测：xxx
	* RNN：可以训练得很好预测出来，根据上下文，不能很好的预测出来
	* 特点：**相关信息和需要该信息的位置之间的距离可能非常的远，此时需要更多的上下文信息才能很好的预测**
* **随着距离的增大，RNN对于将这些信息连接起来无能为力** 
	* 比如根据x0,x1预测h3，可以做到；但是预测h(t)时刻的，如果要用到x0,x1的信息，RNN就很难了。[![20190907164525](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907164525.png)](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907164525.png) 

---

### LSTM：长短期记忆网络

* LSTM：long short term memory networks
	* 可学习到长期依赖关系
	* 目前有很多改进的版本
	* 在许多问题上效果很好
* RNN vs LSTM结构：
	* RNN：重复模块结构简单，只有一个tanh层
	* LSTM：也具有链状结构，但是其重复模块复杂，包含4个神经网络，并且相互之间的交互非常特别 [![20190907165353](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907165353.png)](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907165353.png)
* 核心：元胞状态
	* 图中横穿整个元胞顶部的水平线
	* 像传送带，直接穿过整个链
	* 只有一些较小的线性交互
	* 能对元胞状态添加或者删除信息：通过门的结构控制
	* **门：选择性让信息通过的方法，由一个sigmoid神经网络层和一个元素级相乘操作组成** [![20190907170543](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907170543.png)](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907170543.png)

---

### LSTM步骤

* 总共分4个：
	* 遗忘门：决定我们将要从元胞状态中扔掉哪些信息
	* 输入门：决定我们将会把哪些新信息存储到元胞状态中
	* 更新元胞状态：取代遗忘的信息
	* 输出门：基于目前的元胞状态，并且会加入一些过滤 [![20190907171306](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907171306.png)](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907171306.png)

---

### LSTM的变种

* GRU模型：Gated Recurrent Unit
	* 将遗忘门和输入门合并成为单一的“更新门(Update Gate)”
	* 将元胞状态(Cell State)和隐状态(Hidden State)合并
	* 比标准的LSTM模型更简化
	* 越来越流行 ![](https://www.yunaitong.cn/media/14585445702525/14585667357562.png)

---

### pytorch实现单向LSTM

使用单向LSTM：

```python
nn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)#(input_size,hidden_size,num_layers)
input = torch.randn(5, 3, 10)#(seq_len, batch, input_size)
h0 = torch.randn(2, 3, 20) #(num_layers,batch,output_size)
c0 = torch.randn(2, 3, 20) #(num_layers,batch,output_size)
output, (hn, cn) = rnn(input, (h0, c0))

output.shape #(seq_len, batch, output_size)
torch.Size([5, 3, 20])
hn.shape #(num_layers, batch, output_size)
torch.Size([2, 3, 20])
cn.shape #(num_layers, batch, output_size)
torch.Size([2, 3, 20])
```

[![20190907172256](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907172256.png)](https://raw.githubusercontent.com/Tsinghua-gongjing/blog_codes/master/images/20190907172256.png)

---

### pytorch实现双向LSTM

```python
rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2,bidirectional=True)#(input_size,hidden_size,num_layers)
input = torch.randn(5, 3, 10)#(seq_len, batch, input_size)
h0 = torch.randn(4, 3, 20) #(num_layers,batch,output_size)
c0 = torch.randn(4, 3, 20) #(num_layers,batch,output_size)
output, (hn, cn) = rnn(input, (h0, c0))

output.shape #(seq_len, batch, output_size*2)
torch.Size([5, 3, 40])
hn.shape #(num_layers*2, batch, output_size)
torch.Size([4, 3, 20])
cn.shape #(num_layers*2, batch, output_size)
torch.Size([4, 3, 20])
```

![](http://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f7bdb849dafe17c952bfd88b879e01f74cf59d78/4-Figure3-1.png)

---

### 例子：使用LSTM对语句中的单词进行词性标注

每个单词可以有多个词性，在不同的语境中，其词性是不同的，所以语义分析的一个点就是对任意一句话进行每个单词的词性标注。因为这个是上下文强相关的，所以很适合LSTM模型。

* 输入：一句话，转换后的特征：每个单词在词典中的index值
* 输出：一个矩阵，每个单词所属词性类别的概率，比如一句话总共10个单词，总共有3个词性类别，那么生成的就是一个10x3的矩阵。然后用softmax可以取最大概率值的作为每个词的最后词性标注。

```python
# https://www.pytorchtutorial.com/pytorch-sequence-model-and-lstm-networks/

import torch
import torch.autograd as autograd # torch中自动计算梯度模块
import torch.nn as nn             # 神经网络模块
import torch.nn.functional as F   # 神经网络模块中的常用功能 
import torch.optim as optim       # 模型优化器模块
 
torch.manual_seed(1)

######################
### 定义训练数据
training_data = [
    ("The dog ate the apple".split(), ["DET", "NN", "V", "DET", "NN"]),
    ("Everybody read that book".split(), ["NN", "V", "DET", "NN"])
]

######################
### 创建词典：单词词典用于特征生成；词性词典用于label生成
# 创建索引字典（这里是根据训练数据创建的，一般的应该是有一个大而全的词典）
word_to_ix = {} # 单词的索引字典
for sent, tags in training_data:
    for word in sent:
        if word not in word_to_ix:
            word_to_ix[word] = len(word_to_ix)
print(word_to_ix)

# 创建词性的词典
tag_to_ix = {"DET": 0, "NN": 1, "V": 2} # 手工设定词性标签数据字典

# 根据序列和对应的词典，生成对应序列在词典中的index
# sentence+单词词典 =》sentence顺序每个单词的index
# tags+词性词典 =》每个词的词性对应的index
def prepare_sequence(seq, to_ix):
    idxs = [to_ix[w] for w in seq]
    tensor = torch.LongTensor(idxs)
    return autograd.Variable(tensor)

######################
### 定义LSTM网络
class LSTMTagger(nn.Module):
 
    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):
        super(LSTMTagger, self).__init__()
        self.hidden_dim = hidden_dim
 
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
 
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
 
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)
        self.hidden = self.init_hidden()
 
    def init_hidden(self):
        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),
                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))
 
    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, self.hidden = self.lstm(
            embeds.view(len(sentence), 1, -1), self.hidden)
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_space)
        return tag_scores
    
######################
### 定义一个具体的LSTM网络：主要是对网络参数进行传值
model = LSTMTagger(2, 3, len(word_to_ix), len(tag_to_ix))
# 定义损失函数
loss_function = nn.NLLLoss()
# 定义优化器
optimizer = optim.SGD(model.parameters(), lr=0.1)
 
######################
### 这里是还没有使用训练数据进行模型训练的结果
# inputs = prepare_sequence(training_data[0][0], word_to_ix)
# tag_scores = model(inputs)
# print(training_data[0][0])
# print(inputs)
# print(tag_scores)
# ['The', 'dog', 'ate', 'the', 'apple']
# tensor([0, 1, 2, 3, 4])
# tensor([[-1.6456, -1.0384, -0.7917],
#         [-1.6287, -1.1794, -0.7004],
#         [-1.6278, -1.2331, -0.6689],
#         [-1.6497, -1.1290, -0.7245],
#         [-1.6244, -1.2177, -0.6791]], grad_fn=<LogSoftmaxBackward>)

######################
### 使用训练数据进行模型训练
for epoch in range(300): # 我们要训练300次，可以根据任务量的大小酌情修改次数。
    print("ecpo:",epoch)
    for sentence, tags in training_data:
        # 清除网络先前的梯度值，梯度值是Pytorch的变量才有的数据，Pytorch张量没有
        model.zero_grad()
        # 重新初始化隐藏层数据，避免受之前运行代码的干扰
        model.hidden = model.init_hidden()
        # 准备网络可以接受的的输入数据和真实标签数据，这是一个监督式学习
        # 这里的模型的输入x就是sentence_in，转换后就是句子里每个词在词典里面的index
        sentence_in = prepare_sequence(sentence, word_to_ix)
        targets = prepare_sequence(tags, tag_to_ix)
        # 运行我们的模型，直接将模型名作为方法名看待即可
        tag_scores = model(sentence_in)
        # 计算损失，反向传递梯度及更新模型参数
        loss = loss_function(tag_scores, targets)
        loss.backward()
        optimizer.step()
 
######################
### 检验模型效果：这里用的是训练数据，只为了演示
inputs = prepare_sequence(training_data[0][0], word_to_ix)
tag_scores = model(inputs)
print(tag_scores)
# tensor([[-0.9031, -0.5412, -4.3697],
#         [-2.3633, -0.1098, -4.6217],
#         [-2.7825, -3.1968, -0.1085],
#         [-0.6320, -0.8947, -2.8180],
#         [-2.3956, -0.1101, -4.3304]], grad_fn=<LogSoftmaxBackward>)
```

### 参考

* [理解 LSTM 网络](https://www.yunaitong.cn/understanding-lstm-networks.html)
* [lstm理解与使用(pytorch为例)](https://blog.csdn.net/qq_16949707/article/details/84845552)

---
