---
layout: post
category: "machinelearning"
title:  "CS229 notes"
tags: [python, machine learning]
---

# 课程信息

* Stanford course material: http://cs229.stanford.edu/syllabus.html
* Stanford video: https://see.stanford.edu/course/cs229
* Webpage notes: http://www.holehouse.org/mlclass/
* Coursera video: https://www.coursera.org/learn/machine-learning
* Coursera video slides and quiz on Github (fork from [atinesh-s](https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford)): https://github.com/Tsinghua-gongjing/Coursera-Machine-Learning-Stanford

# 课程笔记

* 注意：`很多介绍的内容都很详细，这里只记录一些自己觉得容易忘记或者难以理解的点`。

## 01 and 02: Introduction, Regression Analysis and Gradient Descent

1. definition: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E . --- Tom Mitchell (1998)
2. supervised learning:
   - supervised learning: "right answers" given
   - regression: predict continuous valued output (e.g., house price)
   - classification: predict discrete valued output (e.g., cancer type)
3. unsupervised learning: 
   - unlabelled data, using various clustering methods to structure it
   - examples: google news, gene expressions, organise computer clusters, social network analysis, astronomical data analysis
   - **cocktail party problem**: overlapped voice, how to separate?
4. linear regression one variable (univariate): 
   - m : number of training examples
   - X's : input variable / features
   - Y's : output variable / target variable
   - cost function: squared error function:
   - ![](http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[9].png)
   - [![linear_regression.jpeg](https://i.loli.net/2019/04/15/5cb468d5c76f0.jpeg)](https://i.loli.net/2019/04/15/5cb468d5c76f0.jpeg)
5. parameter estimation: gradient decent algorithm
   - [![gradient_decent.jpeg](https://i.loli.net/2019/04/15/5cb4691e99e8d.jpeg)](https://i.loli.net/2019/04/15/5cb4691e99e8d.jpeg)

## 03: Linear Algebra - review

1. 概念：
   - matrix: rectangular array of numbers: rows x columns
   - element: i -> ith row, j -> jth column
   - vector: a nx1 matrix
2. 操作：
   - 加和: 需要相同的维，才能元素级别的相加减。
   - 标量乘积
   - 混合运算
   - [![matrix_calculus.jpeg](https://i.loli.net/2019/04/16/5cb53ac91b736.jpeg)](https://i.loli.net/2019/04/16/5cb53ac91b736.jpeg)
 

## 04: Linear Regression with Multiple Variables

1. 多特征使得fitting函数变得更复杂，多元线性回归。
2. 多元线性回归的损失函数：
   - ![](http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image.png)
3. 多变量的梯度递减：
   - ![](http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image%20[1].png)
   - 规则1：feature scaling。对于不同的feature范围，通过特征缩减到可比较的范围，通常[-1, 1]之间。
   - 归一化：1）除以各自特征最大值；2）mean normalization(如下)：
   - ![](http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image%20[6].png)
   - 规则2：learning rate。选取合适的学习速率，太小则收敛太慢，太大则损失函数不一定会随着迭代次数减小（甚至不收敛）。
   - 损失函数曲线：直观判断训练多久时模型会达到收敛
   - ![](http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image%20[7].png)
4. 特征和多项式回归：对于非线性问题，也可以尝试用多项式的线性回归，基于已有feature构建额外的特征，比如房间size的三次方或者开根号等，但是要注意与模型是否符合。比如size的三次方作为一个特征，随着size增大到一定值后，其模型输出值是减小的，这显然不符合size越大房价越高。
5. Normal equation：根据损失函数，求解最小损失岁对应的theta向量的，类似于求导，但是这里采用的是矩阵运算的方式。
   - 求解方程式如下：
   - ![](http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image%20[13].png)
   - 这里就**直接根据训练集和label值矩阵求解出最小损失对对应的各个参数（权重）**。
6. 什么时候用梯度递减，什么时候用normal equation去求解最小损失函数处对应的theta向量？
   - [![gradient_decent_vs_normal_equation.jpeg](https://i.loli.net/2019/04/17/5cb60508a1790.jpeg)](https://i.loli.net/2019/04/17/5cb60508a1790.jpeg)

## 05: Octave[incomplete]

## 06: Logistic Regression

1. 逻辑回归：分类，比如email是不是垃圾邮件，肿瘤是不是恶性的。预测y值（label），=1（positive class），=0（negative class）。
2. 分类 vs 逻辑回归（逻辑回归转换为分类）：
   - 分类：值为0或1（是离散的，且只能取这两个值）。
   - 逻辑回归：预测值在[0,1之间]。
   - 阈值法：用逻辑回归模型，预测值>=0.5，则y=1，预测值<0.5，则y=0.
3. 逻辑回归函数（假设，hypothesis）：
   - 公式：![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[1].png)
   - 分布：![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[2].png)
4. 决策边界（decision boundary）：区分概率值（0.5）对应的theta值=0，所以函数=0所对应的线。
   - 线性区分的边界：![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[5].png)
   - 非线性区分的边界：![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[6].png)
5. 损失函数:
   - 问题：
   - ![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[8].png)
   - 如果延续线性函数的损失函数，则可写成如下，但是当把逻辑函数代入时，这个损失函数是一个非凸优化（non-convex，有很多局部最优，难以找到全局最优）的函数。
   - ![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[9].png)
   - 因此，需要使用一个凸函数作为逻辑函数的损失函数：
   - ![](http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[12].png)

## 07: Regularization

1. 过拟合的问题：
  - 线性过拟合：预测房价的问题，从一阶到二阶到四阶的线性拟合【之前的学习也知道，如果模型中的特征数目很多，那么损失函数有可能越接近于0】，损失越来越小大，但是缺乏泛化到新数据的能力。
  - 欠拟合（underfitting）：高偏差。
  - 过拟合（overfitting）：高方差，假设空间太大。
  - 逻辑回归的过拟合：其函数经过逻辑函数之前可以简单或者复杂，从而欠拟合或者过拟合。
  - [![overfitting_and_underfitting.png](https://i.loli.net/2019/04/22/5cbd84635925f.png)](https://i.loli.net/2019/04/22/5cbd84635925f.png)
2. 如何解决过拟合：
  - 如何鉴定是否过拟合？泛化能力很差，对新样本的预测效果很糟糕。
  - 低维时可以画出来，看拟合的好坏？高维时不能很好的展示。
  - 特征太多，数据太少容易过拟合。
  - 方案【1】减少特征数目。1）手动挑选特征；2）算法模型挑选；3）挑选特征会带来信息丢失
  - 方案【2】正则化。1）保留所有特征，但是减小权重函数的量级；2）当有很多特征时，每一个特征对于预测都贡献一点点。
4. 正则化：
  - 参数值较小时模型越简单
  - 简单的模型更不容易过拟合
  - 加入正则项，减小每个参数的值
  - 加入正则项后的损失函数：
  - ![](http://www.holehouse.org/mlclass/07_Regularization_files/Image%20[4].png)
5. λ正则化参数：平衡模型对于训练数据的拟合程度，和所有参数趋于小（模型趋向于简单）
  - 如果λ很大，所有的参数就都很小，各个特征项没啥用，模型预测效果不好 =》欠拟合。
  - [![cost_function_with_regularization.png](https://i.loli.net/2019/04/22/5cbd9cd87ef8a.png)](https://i.loli.net/2019/04/22/5cbd9cd87ef8a.png)

## 08: Neural Networks - Representation

1. 非线性问题：线性不可分，增加各种特征使得可分。比如根据图片检测汽车（计算机视觉）。当特征空间很大时，逻辑回归不再适用，而神经网络则是一个更好的非线性模型。
2. 神经网络：想要模拟大脑（不同的皮层区具有不同的功能，如味觉、听觉、触觉等），上世纪80-90年代很流行，90年达后期开始没落，现在又很流行，解决很多实际的问题。
3. 神经网络：
   - cell body, input wires (dendrities, 树突), output wire (axon，轴突)
3. 逻辑单元：最简单的神经元。一个输入层，一个激活函数，一个输出层。
4. 神经网络：激活函数，权重矩阵：
   - 输入层，输出层，隐藏层
   - ai(j) - activation of unit i in layer j 
   - ![](http://www.holehouse.org/mlclass/08_Neural_Networks_Representation_files/Image%20[7].png)
5. 前向传播：向量化实现，使用向量表示每一层次的输出。
6. 使用神经网络实现逻辑符号（逻辑与、逻辑或，逻辑和）：
   - 实现的是逻辑，而非线性问题，所以神经网络能很好的用于非线性问题上。
   - 下面的是实现 XNOR （NOT XOR）：
   - ![](http://www.holehouse.org/mlclass/08_Neural_Networks_Representation_files/Image%20[17].png)
7. 多分类问题：one-vs-all

## 09: Neural Networks - Learning

1. 神经网络分类问题：
   - 二分类：输出为0或1
   - 多分类：比如有k个类别，则输出一个向量（长度为k，独热编码表示）
   - 损失函数：类比逻辑回归的损失函数
   - 逻辑回归的损失函数(对数损失+权重参数的正则项)：
   - ![](http://www.holehouse.org/mlclass/09_Neural_Networks_Learning_files/Image%20[2].png)
   - 神经网络的损失函数：
   - 注意1：输出有k个节点，所以需要对所有的节点进行计算
   - 注意2：第一部分，所有节点的平均逻辑对数损失
   - 注意3：第二部分，正则和（又称为weight decay），只不过是所有参数的
   - ![](http://www.holehouse.org/mlclass/09_Neural_Networks_Learning_files/Image%20[3].png)
2. 前向传播（forward propagation）：
   - 训练样本、结果已知
   - 每一层的权重可以用theta向量表示，这也是需要确定优化的参数
   - 每一层的激活函数已知
   - 就可以根据以上的数据和参数一层一层的计算每个节点的值，并与已知的值进行比较，构建损失函数
3. 反向传播（back propagation）：
   - 每一层的每个节点都会计算出一个值，但是这个值与真实值是有差异的，因此可以计算每个节点的错误。
   - 但是每个节点的真实值我们是不知道的，只知道最后的y值（输出值），因此需要从最后的输出值开始计算。
   - [这个文章: 一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html)通过一个简单的3层网络的计算，演示了反向传播的过程，可以参考一下：
   - [![back_propagation1.jpeg](https://i.loli.net/2019/04/28/5cc48a65ea9a6.jpeg)](https://i.loli.net/2019/04/28/5cc48a65ea9a6.jpeg)
   - [![back_propagation2.jpeg](https://i.loli.net/2019/04/28/5cc48a65e77e1.jpeg)](https://i.loli.net/2019/04/28/5cc48a65e77e1.jpeg)
   
4. 神经网络学习：
   - [![neural_network_training.png](https://i.loli.net/2019/04/27/5cc401672148f.png)](https://i.loli.net/2019/04/27/5cc401672148f.png)

## 10: Advice for applying machine learning techniques

## 11: Machine Learning System Design

## 12: Support Vector Machines

## 13: Clustering

## 14: Dimensionality Reduction

## 15: Anomaly Detection

## 16: Recommender Systems

## 17: Large Scale Machine Learning

## 18: Application Example - Photo OCR

## 19: Course Summary